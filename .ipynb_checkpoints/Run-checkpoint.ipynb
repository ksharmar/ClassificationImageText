{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0   a   apple   1,2,3,4\n",
    "# 1   b   banana  7,1,2,3\n",
    "# 2   c   apple   1,2,3,11\n",
    "# 3   d   banana  7,2,4,6\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df[0] = [0, 1, 2, 3]\n",
    "df[1] = ['a','b', 'c','d']\n",
    "df[2] = ['apple', 'banana', 'apple', 'banana']\n",
    "df[3] = ['1,2,3,4', '7,1,2,3', '1,2,3,11', '7,2,4,6']\n",
    "df\n",
    "df.to_csv('input.txt', sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_path='./input.txt', list_alpha=[0.5, 0.6], list_b=[1.0], list_bf=[1.0, 1.5], list_e=[1.0], list_k=[2, 3], save_path_dir='./saved_test/')\n",
      "starting noise ranking..\n",
      "Index(['sample_key', 'content_url', 'class_name', 'features'], dtype='object')\n",
      "Starting nearest neighbor search..\n",
      "Indexing dataset.. 4\n",
      "Search for max k=4 neighbors\n",
      "knn Search completed (4, 4)\n",
      "--------------Starting noise ranking with params=k=2_a=0.5_bf=1.0_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 0 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 2)\n",
      "Starting aggregation of scores..\n",
      ">>elem=7\n",
      "Done aggregation.. 3\n",
      "--------------Starting noise ranking with params=k=2_a=0.5_bf=1.5_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 0 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 2)\n",
      "Starting aggregation of scores..\n",
      ">>elem=7\n",
      "Done aggregation.. 3\n",
      "--------------Starting noise ranking with params=k=2_a=0.6_bf=1.0_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 0 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 2)\n",
      "Starting aggregation of scores..\n",
      ">>elem=7\n",
      "Done aggregation.. 3\n",
      "--------------Starting noise ranking with params=k=2_a=0.6_bf=1.5_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 0 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 2)\n",
      "Starting aggregation of scores..\n",
      ">>elem=7\n",
      "Done aggregation.. 3\n",
      "--------------Starting noise ranking with params=k=3_a=0.5_bf=1.0_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 1 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 3)\n",
      "Starting aggregation of scores..\n",
      ">>elem=11\n",
      "Done aggregation.. 4\n",
      "--------------Starting noise ranking with params=k=3_a=0.5_bf=1.5_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 1 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 3)\n",
      "Starting aggregation of scores..\n",
      ">>elem=11\n",
      "Done aggregation.. 4\n",
      "--------------Starting noise ranking with params=k=3_a=0.6_bf=1.0_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 1 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 3)\n",
      "Starting aggregation of scores..\n",
      ">>elem=11\n",
      "Done aggregation.. 4\n",
      "--------------Starting noise ranking with params=k=3_a=0.6_bf=1.5_b=1.0_e=1.0----------\n",
      "Starting weighted knn classifier to get pred_labels..\n",
      ">>instance=3, pred_label=1\n",
      "Done predicted labels.. [1 1 1 1]\n",
      "Assign blame/reward scores to neighbors acc to clique types..\n",
      ">>instance=3 with given label=1 and pred_label=1\n",
      "Done score assignment by clique types.. (4, 3)\n",
      "Starting aggregation of scores..\n",
      ">>elem=11\n",
      "Done aggregation.. 4\n",
      "finished noise ranking..\n"
     ]
    }
   ],
   "source": [
    "%run -i \"main.py\" \\\n",
    "--input_path='./input.txt' \\\n",
    "--list_k 2 3 \\\n",
    "--list_alpha 0.5 0.6 --list_bf 1.0 1.5 \\\n",
    "--list_b 1 --list_e 1 \\\n",
    "--save_path_dir='./saved_test/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(results_path_dir='./saved_test/', validation_path='./input2.txt')\n",
      "Starting evaluation on validation verified labels..\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=3_a=0.5_bf=1.0_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (1.0, 0.5, 0.6666666666666666, None)\n",
      "f1_metrics (macro/unweighted mean) 0.7333333333333334\n",
      "avg accuracy over classes 0.75 AvgErrorRate 0.25\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.75 ErrorRate 0.25\n",
      "P/R/F1 (clean) (0.6666666666666666, 1.0, 0.8, None)\n",
      "f1_metrics (weighted mean of f1) 0.7333333333333334\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=3_a=0.6_bf=1.0_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (0.0, 0.0, 0.0, None)\n",
      "f1_metrics (macro/unweighted mean) 0.3333333333333333\n",
      "avg accuracy over classes 0.5 AvgErrorRate 0.5\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.5 ErrorRate 0.5\n",
      "P/R/F1 (clean) (0.5, 1.0, 0.6666666666666666, None)\n",
      "f1_metrics (weighted mean of f1) 0.3333333333333333\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=2_a=0.5_bf=1.5_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (1.0, 0.5, 0.6666666666666666, None)\n",
      "f1_metrics (macro/unweighted mean) 0.7333333333333334\n",
      "avg accuracy over classes 0.75 AvgErrorRate 0.25\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.75 ErrorRate 0.25\n",
      "P/R/F1 (clean) (0.6666666666666666, 1.0, 0.8, None)\n",
      "f1_metrics (weighted mean of f1) 0.7333333333333334\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=3_a=0.5_bf=1.5_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (1.0, 0.5, 0.6666666666666666, None)\n",
      "f1_metrics (macro/unweighted mean) 0.7333333333333334\n",
      "avg accuracy over classes 0.75 AvgErrorRate 0.25\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.75 ErrorRate 0.25\n",
      "P/R/F1 (clean) (0.6666666666666666, 1.0, 0.8, None)\n",
      "f1_metrics (weighted mean of f1) 0.7333333333333334\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=2_a=0.6_bf=1.0_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (0.0, 0.0, 0.0, None)\n",
      "f1_metrics (macro/unweighted mean) 0.3333333333333333\n",
      "avg accuracy over classes 0.5 AvgErrorRate 0.5\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.5 ErrorRate 0.5\n",
      "P/R/F1 (clean) (0.5, 1.0, 0.6666666666666666, None)\n",
      "f1_metrics (weighted mean of f1) 0.3333333333333333\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=3_a=0.6_bf=1.5_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (0.0, 0.0, 0.0, None)\n",
      "f1_metrics (macro/unweighted mean) 0.3333333333333333\n",
      "avg accuracy over classes 0.5 AvgErrorRate 0.5\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.5 ErrorRate 0.5\n",
      "P/R/F1 (clean) (0.5, 1.0, 0.6666666666666666, None)\n",
      "f1_metrics (weighted mean of f1) 0.3333333333333333\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=2_a=0.6_bf=1.5_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (0.0, 0.0, 0.0, None)\n",
      "f1_metrics (macro/unweighted mean) 0.3333333333333333\n",
      "avg accuracy over classes 0.5 AvgErrorRate 0.5\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.5 ErrorRate 0.5\n",
      "P/R/F1 (clean) (0.5, 1.0, 0.6666666666666666, None)\n",
      "f1_metrics (weighted mean of f1) 0.3333333333333333\n",
      "\n",
      "\n",
      "-------------------Evaluating file = ./saved_test/k=2_a=0.5_bf=1.0_b=1.0_e=1.0.txt ------------------\n",
      "P/R/F1 (noise) (1.0, 0.5, 0.6666666666666666, None)\n",
      "f1_metrics (macro/unweighted mean) 0.7333333333333334\n",
      "avg accuracy over classes 0.75 AvgErrorRate 0.25\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.75 ErrorRate 0.25\n",
      "P/R/F1 (clean) (0.6666666666666666, 1.0, 0.8, None)\n",
      "f1_metrics (weighted mean of f1) 0.7333333333333334\n",
      "Finished evaluation..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_util.py:57: UserWarning: loadtxt: Empty input file: \"./saved_test/k=3_a=0.6_bf=1.0_b=1.0_e=1.0.txt\"\n",
      "  detected_noise = np.loadtxt(result_file)\n",
      "/home/krsharma/miniconda3/envs/faiss/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/krsharma/miniconda3/envs/faiss/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "eval_util.py:57: UserWarning: loadtxt: Empty input file: \"./saved_test/k=2_a=0.6_bf=1.0_b=1.0_e=1.0.txt\"\n",
      "  detected_noise = np.loadtxt(result_file)\n",
      "eval_util.py:57: UserWarning: loadtxt: Empty input file: \"./saved_test/k=3_a=0.6_bf=1.5_b=1.0_e=1.0.txt\"\n",
      "  detected_noise = np.loadtxt(result_file)\n",
      "eval_util.py:57: UserWarning: loadtxt: Empty input file: \"./saved_test/k=2_a=0.6_bf=1.5_b=1.0_e=1.0.txt\"\n",
      "  detected_noise = np.loadtxt(result_file)\n"
     ]
    }
   ],
   "source": [
    "%run -i \"eval_util.py\" \\\n",
    "--validation_path='./input2.txt' \\\n",
    "--results_path_dir='./saved_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss",
   "language": "python",
   "name": "faiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
