{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# NOTE IN WASEEM_TRAIN.csv indices are ordered by sample_key used in validation csv.\n",
    "\n",
    "K = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "logits = np.loadtxt('../trained_models/waseem/logits_epoch.txt')\n",
    "labels = pd.read_csv('../datasets/waseem/waseem_train.csv', header=0, sep='\\t').label.values\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 818 4694 1357 ...  196 1251  767]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../trained_models/waseem/noiserank_val.tsv', sep='\\t', header=None) # .label.values\n",
    "df.head()\n",
    "val_ids = df[0].values\n",
    "val_vlabels = df[3].values\n",
    "print(val_ids)\n",
    "print(val_vlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------starting for k=1-----------------\n",
      "classes (5527, 1)\n",
      "val_classes (4697, 1)\n",
      "[[3]\n",
      " [1]\n",
      " [3]\n",
      " ...\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "[0. 0. 0. 1.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "P/R/F1 (noise) (0.2890855457227139, 0.14803625377643503, 0.1958041958041958, None)\n",
      "f1_metrics (macro/unweighted mean) 0.5499454673766601\n",
      "avg accuracy over classes 0.48693414488618797 AvgErrorRate 0.5130658551138121\n",
      "\n",
      "f1_metrics (accuracy/micro) 0.8286140089418778 ErrorRate 0.17138599105812224\n",
      "P/R/F1 (clean) (0.8705828361633777, 0.940272614622057, 0.9040867389491243, None)\n",
      "f1_metrics (weighted mean of f1) 0.8042606704879911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, precision_recall_fscore_support\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pprint import pprint\n",
    "\n",
    "def report_metrics(targets, preds, classes):\n",
    "    C = np.max(classes) + 1\n",
    "    per_class_accuracies = np.zeros((C,))\n",
    "    for c in range(C):\n",
    "        ind = np.where(classes == c)[0]\n",
    "        class_acc = f1_score(targets[ind], preds[ind], average='micro')\n",
    "        per_class_accuracies[c] = class_acc\n",
    "    \n",
    "    print('P/R/F1 (noise)', precision_recall_fscore_support(targets, preds, pos_label=0, average='binary'))\n",
    "    print('f1_metrics (macro/unweighted mean)', f1_score(targets, preds, average='macro'))\n",
    "    acc_classes = np.mean(per_class_accuracies)\n",
    "    print('avg accuracy over classes', acc_classes, 'AvgErrorRate', 1 - acc_classes)\n",
    "    \n",
    "    print()\n",
    "    acc = f1_score(targets, preds, average='micro')\n",
    "    print('f1_metrics (accuracy/micro)', acc, 'ErrorRate', 1 - acc)\n",
    "    print('P/R/F1 (clean)', precision_recall_fscore_support(targets, preds, pos_label=1, average='binary'))\n",
    "    print('f1_metrics (weighted mean of f1)', f1_score(targets, preds, average='weighted'))\n",
    "    # cr = classification_report(targets, preds)\n",
    "    \n",
    "\n",
    "def noise_filtering(logits, labels, K, val_ids, val_vlabels):\n",
    "    # rev_sorted = -np.sort(-logits, axis=1)\n",
    "    # items = rev_sorted[:, :K]\n",
    "    # print(items)\n",
    "    nb_cls = len(np.unique(labels))\n",
    "    for k in K:\n",
    "        if k == nb_cls:\n",
    "            break\n",
    "        print('\\n\\n-------------starting for k={}-----------------'.format(k))\n",
    "        \n",
    "        detected_noise = []\n",
    "        preds = []\n",
    "        rev_ind_sorted = np.argsort(-logits, axis=1)\n",
    "        classes = rev_ind_sorted[:, :k]\n",
    "        print('classes', classes.shape)\n",
    "        \n",
    "        # evaluating on validation set:\n",
    "        # binary = np.zeros((len(val_ids), nb_cls))\n",
    "        val_classes = classes[val_ids, :]\n",
    "        val_labels = labels[val_ids]\n",
    "        val_preds = np.zeros((len(val_ids)))\n",
    "        print('val_classes', val_classes.shape)\n",
    "        for i in range(len(val_ids)):\n",
    "            binary = np.zeros((nb_cls))\n",
    "            # binary[i][val_classes[i]] = 1\n",
    "            binary[val_classes[i]] = 1\n",
    "            val_preds[i] = binary[val_labels[i]]  # if its in topK, then pred=1 (clean)\n",
    "        \n",
    "        print(val_classes)\n",
    "        print(binary)\n",
    "        print(val_preds)\n",
    "        report_metrics(val_vlabels, val_preds, val_labels)\n",
    "        break\n",
    "    \n",
    "noise_filtering(logits, labels, K, val_ids, val_vlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
