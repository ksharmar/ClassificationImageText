{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsv = [sample key, image url, class name, verification label, h-dimensional feature delimited by ',']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# emb_file = feat_dir + 'extracted_emb.txt'\n",
    "# img_path_file = feat_dir + 'extracted_imgpaths.txt'\n",
    "\n",
    "# save_train_tsv_file = feat_dir + 'cleannet_train.tsv'\n",
    "# save_val_tsv_file = feat_dir + 'cleannet_val.tsv'\n",
    "# save_all_tsv_file = feat_dir + 'cleannet_all.tsv'\n",
    "# save_class_names_file = feat_dir + 'cleannet_classnames.txt'\n",
    "# np.savetxt(save_class_names_file, tsv['class_name'].unique(), fmt='%s')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 99999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krsharma/miniconda2/envs/env_repl/lib/python2.7/site-packages/ipykernel/__main__.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 199999)\n",
      "(200000, 299999)\n",
      "(300000, 310008)\n"
     ]
    }
   ],
   "source": [
    "## FOOD\n",
    "\n",
    "def create_verified_tsv(type_name):\n",
    "    \n",
    "#     extracted_img_paths_file = \"../trained_models/food101n_20e/extracted_imgpaths.txt\"\n",
    "#     extracted_emb_file = '../trained_models/food101n_20e/extracted_emb.txt'\n",
    "#     strip_field = '../datasets/Food-101N_release/images/'\n",
    "#     if type_name == 'verified_train':\n",
    "#         v_list = \"../datasets/Food-101N_release/meta/verified_train.tsv\"\n",
    "#         save_tsv_file = \"../trained_models/food101n_20e/cleannet_train.tsv\"\n",
    "#         vdf_colnames = ['image_url', 'verification_label']\n",
    "#     elif type_name == 'verified_val':\n",
    "#         v_list = \"../datasets/Food-101N_release/meta/verified_val.tsv\"\n",
    "#         save_tsv_file = \"../trained_models/food101n_20e/cleannet_val.tsv\"\n",
    "#         vdf_colnames = ['image_url', 'verification_label']\n",
    "#     elif type_name == 'all':\n",
    "#         save_tsv_file = \"../trained_models/food101n_20e/cleannet_all.tsv\"\n",
    "        \n",
    "    extracted_img_paths_file = \"../trained_models/round1_food101n_20e/extracted_imgpaths.txt\"\n",
    "    extracted_emb_file = '../trained_models/round1_food101n_20e/extracted_emb.txt'\n",
    "    strip_field = '../datasets/Food-101N_release/images/'\n",
    "    if type_name == 'verified_train':\n",
    "        v_list = \"../datasets/Food-101N_release/meta/verified_train.tsv\"\n",
    "        save_tsv_file = \"../trained_models/round1_food101n_20e/cleannet_train.tsv\"\n",
    "        vdf_colnames = ['image_url', 'verification_label']\n",
    "    elif type_name == 'verified_val':\n",
    "        v_list = \"../datasets/Food-101N_release/meta/verified_val.tsv\"\n",
    "        save_tsv_file = \"../trained_models/round1_food101n_20e/cleannet_val.tsv\"\n",
    "        vdf_colnames = ['image_url', 'verification_label']\n",
    "    elif type_name == 'all':\n",
    "        save_tsv_file = \"../trained_models/round1_food101n_20e/cleannet_all.tsv\"\n",
    "        \n",
    "    extracted_img_paths = pd.read_csv(extracted_img_paths_file, header=None, index_col=None)\n",
    "    extracted_img_paths=extracted_img_paths[0].apply(lambda s: s.split(strip_field)[1])\n",
    "    # print(\"extracted_img_paths\", extracted_img_paths.head())\n",
    "    \n",
    "    if type_name in ['verified_train', 'verified_val']:\n",
    "        vdf = pd.read_csv(v_list, '\\t')\n",
    "        vdf.columns = vdf_colnames\n",
    "        ind = extracted_img_paths.index[extracted_img_paths.isin(vdf['image_url'])].tolist()\n",
    "        print(\"verified df\", vdf.head())\n",
    "        print(\"ind\", ind[0:10])\n",
    "    \n",
    "        # sample_key\timage_url\tclass_name\tverification_label\tfeature\n",
    "        t = pd.DataFrame()\n",
    "        t['sample_key'] = ind\n",
    "        t['image_url'] = np.array(extracted_img_paths.iloc[ind])\n",
    "        t['class_name'] = t['image_url'].map(lambda s: s.split('/')[0])\n",
    "        t = t.merge(vdf)\n",
    "        # iter_csv = pd.read_csv(extracted_emb_file, iterator=True, header=None, chunksize=1000)\n",
    "        # emb = pd.concat([chunk[chunk.index.isin(ind)] for chunk in iter_csv])\n",
    "        a = np.arange(len(extracted_img_paths))\n",
    "        set_ind = set(ind)\n",
    "        skip = [i for i in a if i not in set_ind]\n",
    "        print(\"skip\", skip[0:10])\n",
    "        emb_df = pd.read_csv(extracted_emb_file, skiprows=skip, header=None)\n",
    "        t['feature'] = emb_df[0].apply(lambda e: \",\".join(map(str, e.split())))\n",
    "\n",
    "        t.to_csv(save_tsv_file, sep='\\t', index=False, header=None)\n",
    "        print(\"tsv created\", t.head())\n",
    "    elif type_name == 'all':\n",
    "        # iter_csv = pd.read_csv(extracted_emb_file, iterator=True, header=None, chunksize=1000)\n",
    "        # emb = pd.concat([chunk[chunk.index.isin(ind)] for chunk in iter_csv])\n",
    "        t = pd.DataFrame()\n",
    "        t['sample_key'] = np.arange(len(extracted_img_paths))\n",
    "        t['image_url'] = extracted_img_paths\n",
    "        t['class_name'] = t['image_url'].map(lambda s: s.split('/')[0])\n",
    "        \n",
    "        chunksize=100000\n",
    "        startind = 0\n",
    "        iter_csv = pd.read_csv(extracted_emb_file, iterator=True, header=None, chunksize=chunksize)\n",
    "        for chunk in iter_csv:\n",
    "            write_chunk = pd.DataFrame()\n",
    "            endind = startind+chunksize\n",
    "            if endind > len(extracted_img_paths):\n",
    "                endind = len(extracted_img_paths)\n",
    "            ind = np.arange(startind, endind)\n",
    "            print(ind[0], ind[-1])\n",
    "            startind = endind\n",
    "            write_chunk = t.iloc[ind]\n",
    "            write_chunk['feature'] = chunk[0].apply(lambda e: \",\".join(map(str, e.split())))\n",
    "            # print(\"write_chunk\", write_chunk.head())\n",
    "            with open(save_tsv_file, 'a') as f:\n",
    "                write_chunk.to_csv(f, sep='\\t', index=False, header=None)\n",
    "                # pass\n",
    "            # break\n",
    "    return t\n",
    "\n",
    "t = create_verified_tsv(type_name='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOTHING META FIX\n",
    "\n",
    "# clean_test_kv (image_url, label) 10K\n",
    "# noisy_train_kv (image_url, label) 1m\n",
    "# 47569 clean_train_key_list.txt\n",
    "# 14312 clean_val_key_list.txt\n",
    "\n",
    "# verified_train.tsv 25K (image_url, verilabel)\n",
    "# verified_val.tsv 7K (image_url, verilabel)\n",
    "\n",
    "clean_val_key_list = '../datasets/clothing1m/anno/clean_val_key_list.txt'\n",
    "clean_val = pd.read_csv(clean_val_key_list, sep=' ', header=None)\n",
    "# print(clean_val.head())\n",
    "# print(len(clean_val))\n",
    "\n",
    "\n",
    "clean_train_key_list = '../datasets/clothing1m/anno/clean_train_key_list.txt'\n",
    "clean_train = pd.read_csv(clean_train_key_list, sep=' ', header=None)\n",
    "# print(clean_train.head())\n",
    "# print(len(clean_train))\n",
    "\n",
    "\n",
    "noisy_label_kv_list = '../datasets/clothing1m/anno/noisy_label_kv.txt'\n",
    "noisy_label_kv = pd.read_csv(noisy_label_kv_list, sep=' ', header=None)\n",
    "# print(noisy_label_kv.head())\n",
    "# print(len(noisy_label_kv))\n",
    "\n",
    "clean_label_kv_list = '../datasets/clothing1m/anno/clean_label_kv.txt'\n",
    "clean_label_kv = pd.read_csv(clean_label_kv_list, sep=' ', header=None)\n",
    "# print(clean_label_kv.head())\n",
    "# print(len(clean_label_kv))\n",
    "\n",
    "def veri_table(clean_tab, noisy_kv, clean_kv, name):\n",
    "    v = noisy_kv[noisy_kv[0].isin(clean_tab[0])]\n",
    "    v.columns = ['img_url', 'class_name']\n",
    "    t = v.merge(clean_kv, on='img_url')\n",
    "    t['verification_label'] = (t['class_name'] == t['gt_class'])*1\n",
    "    print(t.head())\n",
    "    print(\"num:\", len(t))\n",
    "    t.to_csv('../datasets/clothing1m/verified_{}.tsv'.format(name), index=False, sep='\\t')\n",
    "    return t\n",
    "    \n",
    "clean_label_kv.columns = ['img_url', 'gt_class']\n",
    "verified_val = veri_table(clean_val, noisy_label_kv, clean_label_kv, 'val')\n",
    "verified_train = veri_table(clean_train, noisy_label_kv, clean_label_kv, 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_val = pd.read_csv('../datasets/clothing1m/verified_{}.tsv'.format('val'), sep='\\t')\n",
    "verified_train = pd.read_csv('../datasets/clothing1m/verified_{}.tsv'.format('train'), sep='\\t')\n",
    "# print(len(verified_val), verified_val.head())\n",
    "# print(len(verified_train), verified_train.head())\n",
    "\n",
    "extra_verified_df = pd.concat([verified_val, verified_train])[['img_url', 'class_name']]\n",
    "# print(extra_verified_df.head())\n",
    "# extra_verified_df.to_csv('../datasets/clothing1m/extra_verified_kv.tsv', index=False, header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('extracted_img_paths', 1032102,                                       0\n",
      "0   images/2/46/50748838,3981129246.jpg\n",
      "1  images/2/56/860908654,4267574256.jpg\n",
      "2  images/9/38/436489045,3143453938.jpg\n",
      "3  images/0/67/486092382,2324083067.jpg\n",
      "4   images/4/55/4029117294,35432455.jpg)\n",
      "('tsv created', 32102,    sample_key                              image_url  class_name  \\\n",
      "0     1000000  images/2/48/2334049070,2826935248.jpg           2   \n",
      "1     1000001  images/4/84/2554049206,3987464484.jpg           6   \n",
      "2     1000002  images/4/39/3843500707,1475408439.jpg          11   \n",
      "3     1000003  images/4/99/3410098674,2546777499.jpg          11   \n",
      "4     1000004   images/5/99/597790164,1391702599.jpg           9   \n",
      "\n",
      "                                             feature  \n",
      "0  0.081,0.149,0.103,0.532,0.259,0.422,0.805,0.59...  \n",
      "1  0.000,0.162,0.056,0.397,0.805,1.394,0.605,0.35...  \n",
      "2  0.201,0.164,0.976,0.534,0.556,0.312,0.733,0.02...  \n",
      "3  0.262,0.261,0.457,0.752,0.532,0.628,0.128,0.21...  \n",
      "4  0.386,0.263,0.814,0.227,0.957,0.104,0.672,0.14...  )\n"
     ]
    }
   ],
   "source": [
    "# CLOTHING\n",
    "    \n",
    "\n",
    "def create_verified_tsv(type_name):\n",
    "    \n",
    "    extracted_img_paths_file = \"../trained_models/clothing1m_10e/extracted_imgpaths.txt\"\n",
    "    extracted_emb_file = '../trained_models/clothing1m_10e/extracted_emb.txt'  \n",
    "    # class_name of 1st 1m are in noisy_train_v, rest are in verified df directly\n",
    "    noisy_classlabels_file = '../datasets/clothing1m/noisy_train_kv.txt'\n",
    "    \n",
    "    if type_name == 'verified_train':\n",
    "        v_list = \"../datasets/clothing1m/verified_train.tsv\"\n",
    "        save_tsv_file = \"../trained_models/clothing1m_10e/cleannet_train.tsv\"\n",
    "        vdf_colnames = ['image_url', 'class_name', 'gt_class', 'verification_label']\n",
    "    elif type_name == 'verified_val':\n",
    "        v_list = \"../datasets/clothing1m/verified_val.tsv\"\n",
    "        save_tsv_file = \"../trained_models/clothing1m_10e/cleannet_val.tsv\"\n",
    "        vdf_colnames = ['image_url', 'class_name', 'gt_class', 'verification_label']\n",
    "    elif type_name == 'all_1m':\n",
    "        save_tsv_file = \"../trained_models/clothing1m_10e/cleannet_all_1m.tsv\"\n",
    "    elif type_name == 'all_veri':\n",
    "        cleannet_val_file = '../trained_models/clothing1m_10e/cleannet_val.tsv'\n",
    "        cleannet_train_file = '../trained_models/clothing1m_10e/cleannet_train.tsv'\n",
    "        save_tsv_file = \"../trained_models/clothing1m_10e/cleannet_all_veri.tsv\"\n",
    "        \n",
    "    extracted_img_paths = pd.read_csv(extracted_img_paths_file, header=None, index_col=None, sep='\\t')\n",
    "    print(\"extracted_img_paths\", len(extracted_img_paths), extracted_img_paths.head())\n",
    "\n",
    "    if type_name in ['verified_train', 'verified_val']:\n",
    "        vdf = pd.read_csv(v_list, '\\t')\n",
    "        vdf.columns = vdf_colnames\n",
    "        print(\"verified df\", vdf.head())\n",
    "        \n",
    "        ind = extracted_img_paths.index[extracted_img_paths[0].isin(vdf['image_url'])].tolist()\n",
    "        print(\"ind\", ind[0:10])\n",
    "        \n",
    "        # sample_key\timage_url\tclass_name\tverification_label\tfeature\n",
    "        t = pd.DataFrame()\n",
    "        t['sample_key'] = ind\n",
    "        t['image_url'] = np.array(extracted_img_paths.iloc[ind])\n",
    "        t = t.merge(vdf)\n",
    "        # iter_csv = pd.read_csv(extracted_emb_file, iterator=True, header=None, chunksize=1000)\n",
    "        # emb = pd.concat([chunk[chunk.index.isin(ind)] for chunk in iter_csv])\n",
    "        a = np.arange(len(extracted_img_paths))\n",
    "        set_ind = set(ind)\n",
    "        skip = [i for i in a if i not in set_ind]\n",
    "        print(\"skip\", skip[0:10])\n",
    "        emb_df = pd.read_csv(extracted_emb_file, skiprows=skip, header=None)\n",
    "        t['feature'] = emb_df[0].apply(lambda e: \",\".join(map(str, e.split())))\n",
    "        t = t[['sample_key', 'image_url', 'class_name', 'verification_label', 'feature']]\n",
    "        t.to_csv(save_tsv_file, sep='\\t', index=False, header=None)\n",
    "        print(\"tsv created\", t.head())\n",
    "    elif type_name=='all_1m':\n",
    "        t = pd.DataFrame()\n",
    "        t['sample_key'] = np.arange(len(extracted_img_paths))\n",
    "        t['image_url'] = extracted_img_paths\n",
    "        noisy_classlabels = pd.read_csv(noisy_classlabels_file, sep=' ', header=None)\n",
    "        noisy_classlabels.columns = ['image_url', 'class_name']\n",
    "        noisy_classlabels['class_name'].apply(lambda i: str(i))\n",
    "        t['class_name'] = noisy_classlabels['class_name']  # already ordered\n",
    "        # since we don't shuffle batches and read images from filelist into data loader of feat extractor  \n",
    "        chunksize=100000\n",
    "        startind = 0\n",
    "        iter_csv = pd.read_csv(extracted_emb_file, iterator=True, header=None, chunksize=chunksize)\n",
    "        for chunk in iter_csv:\n",
    "            write_chunk = pd.DataFrame()\n",
    "            endind = startind+chunksize\n",
    "            if endind > len(extracted_img_paths):\n",
    "                endind = len(extracted_img_paths)\n",
    "            ind = np.arange(startind, endind)\n",
    "            print(ind[0], ind[-1])\n",
    "            startind = endind\n",
    "            write_chunk = t.iloc[ind]\n",
    "            write_chunk['feature'] = chunk[0].apply(lambda e: \",\".join(map(str, e.split())))\n",
    "            # print(\"write_chunk\", write_chunk.head())\n",
    "            with open(save_tsv_file, 'a') as f:\n",
    "                write_chunk.to_csv(f, sep='\\t', index=False, header=None)\n",
    "                # pass\n",
    "            # break\n",
    "    elif type_name=='all_veri':\n",
    "        t_val = pd.read_csv(cleannet_val_file, sep='\\t', header=None)\n",
    "        t_train = pd.read_csv(cleannet_train_file, sep='\\t', header=None)\n",
    "        write_colnames = ['sample_key', 'image_url', 'class_name', 'feature']\n",
    "        t = pd.concat([t_val, t_train])\n",
    "        t.columns = ['sample_key', 'image_url', 'class_name', 'verification_label', 'feature'] \n",
    "        t = t[write_colnames]\n",
    "        t.to_csv(save_tsv_file, sep='\\t', index=False, header=None)\n",
    "        print(\"tsv created\", len(t), t.head())\n",
    "       \n",
    "    return t\n",
    "\n",
    "t = create_verified_tsv(type_name='all_veri')\n",
    "# cat cleannet_all_1m.tsv cleannet_all_veri.tsv > cleannet_all.tsv\n",
    "# similar to extracted files (imgpaths, emb) creation from subsets (noisy1m, verified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
