{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food format for denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(input_path, out_path):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(input_path, sep='\\t')\n",
    "    df['class_name'] = df['image_name'].apply(lambda x: x.split('/')[0])\n",
    "    newdf = df[['image_name', 'class_name']]\n",
    "    newdf.to_csv(out_path, index=False, header=None, sep=' ')\n",
    "    \n",
    "format('../../trained_models/knn-food-result/food101n_k250_50Kremoved.tsv', '../../trained_models/food101n_20e/knn250_denoised_kv.txt')\n",
    "format('../../trained_models/knn-food-result/food101n_k150_50Kremoved.tsv', '../../trained_models/food101n_20e/knn150_denoised_kv.txt')\n",
    "format('../../trained_models/knn-food-result/food101n_k50_50Kremoved.tsv', '../../trained_models/food101n_20e/knn50_denoised_kv.txt')\n",
    "format('../../trained_models/knn-food-result/food101n_k10_50Kremoved.tsv', '../../trained_models/food101n_20e/knn10_denoised_kv.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothing format for denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format(input_path, out_path):\n",
    "    import pandas as pd\n",
    "    train_1m = pd.read_csv('../../datasets/clothing1m/noisy_train_kv.txt', sep=' ', header=None)\n",
    "    # print(train_1m.head())\n",
    "    # remove those in input_path\n",
    "    df = pd.read_csv(input_path, sep='\\t')\n",
    "    # print(df.head())\n",
    "    clean_set = set(df['image_name'])\n",
    "    newdf = train_1m[train_1m[0].isin(clean_set)]\n",
    "    # print(newdf.head())\n",
    "    # print(len(train_1m), len(clean_set), len(newdf))\n",
    "    newdf.to_csv(out_path, index=False, header=None, sep=' ')\n",
    "    \n",
    "#     df['class_name'] = df['image_name'].apply(lambda x: x.split('/')[0])\n",
    "#     newdf = df[['image_name', 'class_name']]\n",
    "#     newdf.to_csv(out_path, index=False, header=None, sep=' ')\n",
    "    \n",
    "format('../../trained_models/knn-clothing-result/clothing1m_k250_100Kremoved.tsv', '../../trained_models/clothing1m_10e/knn250_denoised_kv_train.txt')\n",
    "# format('../../trained_models/knn-food-result/food101n_k150_50Kremoved.tsv', '../../trained_models/food101n_20e/knn150_denoised_kv.txt')\n",
    "# format('../../trained_models/knn-food-result/food101n_k50_50Kremoved.tsv', '../../trained_models/food101n_20e/knn50_denoised_kv.txt')\n",
    "# format('../../trained_models/knn-food-result/food101n_k10_50Kremoved.tsv', '../../trained_models/food101n_20e/knn10_denoised_kv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  6 12 ... 12 10  6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]),\n",
       " array([ 845,  636,  527, 1171,  918,  824,  558,  297, 1017,  727,  523,\n",
       "         686,  946,  851]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('../../datasets/clothing1m/clean_test_kv.txt', sep=' ', header=None)\n",
    "d.head()\n",
    "\n",
    "c = np.array(d[1])\n",
    "print(c)\n",
    "np.unique(c, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food and Clothing KNN BASELINE P-R eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_clean 260009\n",
      "done preds\n",
      "3824 4741 3991 4741\n",
      "P/R/F1 (noise) (0.2693333333333333, 0.2202835332606325, 0.24235152969406118, None)\n",
      "f1_metrics (macro/unweighted mean) 0.5403696228124817\n",
      "avg accuracy over classes 0.7336881730935796 AvgErrorRate 0.2663118269064204\n",
      "-------------\n",
      "done\n",
      "num_clean 907465\n",
      "done preds\n",
      "4591 7465 6580 7465\n",
      "P/R/F1 (noise) (0.3525423728813559, 0.10855949895615867, 0.1660015961691939, None)\n",
      "f1_metrics (macro/unweighted mean) 0.44268211578220684\n",
      "avg accuracy over classes 0.5668072318642982 AvgErrorRate 0.43319276813570184\n",
      "-------------\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92535"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(images, vlabels, clean_images, classes):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import f1_score, classification_report, precision_recall_fscore_support\n",
    "    \n",
    "    set_clean = set(clean_images)\n",
    "\n",
    "    preds = []\n",
    "    for i in range(len(images)):\n",
    "        img = images[i]\n",
    "        vlabel = vlabels[i]\n",
    "        # print(img, vlabel)\n",
    "        if img in set_clean:\n",
    "            preds.append(1)\n",
    "        else:\n",
    "            preds.append(0)\n",
    "        # break\n",
    "    \n",
    "    print('done preds')\n",
    "    targets = np.array(vlabels)\n",
    "    preds = np.array(preds)\n",
    "    print(np.sum(targets), len(targets), np.sum(preds), len(preds))\n",
    "    \n",
    "    C = np.max(classes) + 1\n",
    "    per_class_accuracies = np.zeros((C,))\n",
    "    for c in range(C):\n",
    "        ind = np.where(classes == c)[0]\n",
    "        class_acc = f1_score(targets[ind], preds[ind], average='micro')\n",
    "        per_class_accuracies[c] = class_acc\n",
    "    \n",
    "    print('P/R/F1 (noise)', precision_recall_fscore_support(targets, preds, pos_label=0, average='binary'))\n",
    "    print('f1_metrics (macro/unweighted mean)', f1_score(targets, preds, average='macro'))\n",
    "    acc_classes = np.mean(per_class_accuracies)\n",
    "    print('avg accuracy over classes', acc_classes, 'AvgErrorRate', 1 - acc_classes)\n",
    "    \n",
    "    report_additional = False\n",
    "    if report_additional:\n",
    "        print()\n",
    "        acc = f1_score(targets, preds, average='micro')\n",
    "        print('f1_metrics (accuracy/micro)', acc, 'ErrorRate', 1 - acc)\n",
    "        print('P/R/F1 (clean)', precision_recall_fscore_support(targets, preds, pos_label=1, average='binary'))\n",
    "        print('f1_metrics (weighted mean of f1)', f1_score(targets, preds, average='weighted'))\n",
    "    # cr = classification_report(targets, preds)\n",
    "\n",
    "\n",
    "def calculate_label_noise_accuracy(clean_file, val_file):\n",
    "    clean_df = pd.read_csv(clean_file, sep='\\t', header=0)\n",
    "    print('num_clean', len(clean_df))\n",
    "    val_df = pd.read_csv(val_file, sep='\\t', header=None)[[0, 1, 2, 3]]\n",
    "    classes = val_df[2]\n",
    "    sclasses = np.sort(np.unique(classes))\n",
    "    cmap = dict(zip(sclasses, np.arange(len(sclasses))))\n",
    "    classes = val_df[2].apply(lambda x: cmap[x])\n",
    "    vlabels = val_df[3]\n",
    "    images = val_df[1]\n",
    "    clean_images = clean_df['image_name']\n",
    "    evaluate(images, vlabels, clean_images, classes)\n",
    "    print('-------------')\n",
    "    return images, vlabels, clean_images, classes\n",
    "\n",
    "images, vlabels, clean_images, classes = calculate_label_noise_accuracy('../../trained_models/knn-food-result/food101n_k250_50Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/food101n_20e/cleannet_val.tsv')\n",
    "\n",
    "# calculate_label_noise_accuracy('../../trained_models/knn-food-result/food101n_k150_50Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/food101n_20e/cleannet_val.tsv')\n",
    "\n",
    "# calculate_label_noise_accuracy('../../trained_models/knn-food-result/food101n_k50_50Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/food101n_20e/cleannet_val.tsv')\n",
    "\n",
    "# calculate_label_noise_accuracy('../../trained_models/knn-food-result/food101n_k10_50Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/food101n_20e/cleannet_val.tsv')\n",
    "\n",
    "print('done')\n",
    "\n",
    "\n",
    "calculate_label_noise_accuracy('../../trained_models/knn-clothing-result/clothing1m_k250_100Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/clothing1m_10e/cleannet_val.tsv')\n",
    "# calculate_label_noise_accuracy('../../trained_models/knn-clothing-result/clothing1m_k150_100Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/clothing1m_10e/cleannet_val.tsv')\n",
    "# calculate_label_noise_accuracy('../../trained_models/knn-clothing-result/clothing1m_k50_100Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/clothing1m_10e/cleannet_val.tsv')\n",
    "# calculate_label_noise_accuracy('../../trained_models/knn-clothing-result/clothing1m_k10_100Kremoved.tsv', '/home/krsharma/ClassificationImageText/trained_models/clothing1m_10e/cleannet_val.tsv')\n",
    "\n",
    "print('done')\n",
    "\n",
    "0.2663, 0.220, 0.2423, 0.5403\n",
    "310000 - 260009\n",
    "\n",
    "0.433, 0.109, 0.166, 0.442\n",
    "1000000 - 907465\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mic",
   "language": "python",
   "name": "mic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
